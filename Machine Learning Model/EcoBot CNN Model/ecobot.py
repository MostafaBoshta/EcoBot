# -*- coding: utf-8 -*-
"""ECOBOT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11cQv3s4yUfMQPI5pfOkNAWqi3HrODPWU
"""

#Marine debris training  on Nasa noaa  & unsplash & other randome free images
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from sklearn.metrics import classification_report
from tensorflow.keras import preprocessing
from keras.layers import Input, Conv2DTranspose
from tensorflow.keras import utils
import matplotlib.pyplot as plot
import tensorflow as tf
import numpy as np
import pandas as pd
import PIL as image
import PIL
import datetime
import zipfile
import pathlib
import time
import os

ECOBOT_Dir = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/ECOBOT dataset/Training')
image_count = len(list(ECOBOT_Dir.glob('*/*.jpg')))
print(image_count)

#loading images
batch_size = 32
img_h = 256
img_w = 256
train_ds = tf.keras.preprocessing.image_dataset_from_directory(ECOBOT_Dir,validation_split=0.2
                                                              ,subset="training",seed=123,
                                                             batch_size = batch_size, image_size=(img_h,img_w))

batch_size = 32
img_h =256
img_w = 256
validation_ds = tf.keras.preprocessing.image_dataset_from_directory(ECOBOT_Dir,validation_split=0.2
                                                              ,subset="validation",seed=123,
                                                             batch_size = batch_size, image_size=(img_h,img_w))

class_names = train_ds.class_names
print(class_names)

plot.figure(figsize=(10, 10))
for images, labels in train_ds.take(10):
    for i in range(9):
        ax = plot.subplot(3, 3, i + 1)
        plot.imshow(images[i].numpy().astype("uint8"))
        plot.title(class_names[labels[i]])
        plot.axis("off")

#normalizing images
normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))

#Aututuning data 
ds_autotune = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size=ds_autotune)
validation_ds = validation_ds.cache().prefetch(buffer_size=ds_autotune)

#creating CNN model
#sequential model
ECOBOT_model = tf.keras.models.Sequential()
#cnn creating concolution layers
#1st layer dense
ECOBOT_model.add(tf.keras.layers.Dense(units = 128,activation='relu',input_shape=(256,256,3)))
ECOBOT_model.add(tf.keras.layers.Dropout(0.2))
#2nd layer convolutional layer
ECOBOT_model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size=3, padding="same", activation ='sigmoid'))
#3rd layer pool
ECOBOT_model.add(tf.keras.layers.MaxPool2D(pool_size =2,strides=2,padding='valid'))
#4th& 5th layers are convolution layer
ECOBOT_model.add(tf.keras.layers.Conv2D(filters = 64,activation='sigmoid',kernel_size=3,padding="same"))
ECOBOT_model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding="same",activation='relu'))
#6th layer is max pool
ECOBOT_model.add(tf.keras.layers.MaxPool2D(pool_size=2,padding='valid',strides=2))
#7th layer
ECOBOT_model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding="same",activation='relu'))
#

#output layer softmax
ECOBOT_model.add(tf.keras.layers.Dense(units=10,activation='softmax'))
ECOBOT_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
                      metrics=['sparse_categorical_accuracy'])
ECOBOT_model.add(tf.keras.layers.Flatten())

ECOBOT_model.summary()
def elapsed_time(seconds):
    return str(datetime.timedelta(seconds = t))
t = time.time()
print(elapsed_time(t))
ECOBOT_model.fit(train_ds,validation_data = validation_ds,epochs=5)
def elapsed_time(seconds):
    return str(datetime.timedelta(seconds = t))
t = time.time()
print(elapsed_time(t))

ECOBOT_model.evaluate(validation_ds)

